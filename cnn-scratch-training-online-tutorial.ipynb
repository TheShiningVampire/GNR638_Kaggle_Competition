{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Load libraries\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import glob\r\n",
    "import torch.nn as nn\r\n",
    "from torchvision.transforms import transforms\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.optim import Adam\r\n",
    "from torch.autograd import Variable\r\n",
    "import torchvision\r\n",
    "import pathlib"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#checking for device\r\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "## Hyperparameters\r\n",
    "batch_size_ = 2\r\n",
    "num_epochs = 50\r\n",
    "size = 150"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Transforms\r\n",
    "transformer=transforms.Compose([\r\n",
    "    transforms.Resize((size,size)),\r\n",
    "    transforms.RandomHorizontalFlip(),\r\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\r\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\r\n",
    "                        [0.5,0.5,0.5])\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Dataloader\r\n",
    "\r\n",
    "#Path for training and testing directory\r\n",
    "train_path='Dataset\\\\train'\r\n",
    "test_path='Dataset\\\\val'\r\n",
    "\r\n",
    "train_loader=DataLoader(\r\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\r\n",
    "    batch_size=batch_size_, shuffle=True\r\n",
    ")\r\n",
    "test_loader=DataLoader(\r\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\r\n",
    "    batch_size=batch_size_, shuffle=True\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#categories\r\n",
    "root=pathlib.Path(train_path)\r\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(classes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['basketball_court', 'bridge', 'crosswalk', 'golf_course', 'oil_well', 'overpass', 'railway', 'runway', 'swimming_pool', 'tennis_court']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#CNN Network\r\n",
    "\r\n",
    "\r\n",
    "class ConvNet(nn.Module):\r\n",
    "    def __init__(self,num_classes):\r\n",
    "        super(ConvNet,self).__init__()\r\n",
    "        \r\n",
    "        #Output size after convolution filter\r\n",
    "        #((w-f+2P)/s) +1\r\n",
    "        \r\n",
    "        #Input shape= (256,3,150,150)\r\n",
    "        \r\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\r\n",
    "        #Shape= (256,12,150,150)\r\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\r\n",
    "        #Shape= (256,12,150,150)\r\n",
    "        self.relu1=nn.ReLU()\r\n",
    "        #Shape= (256,12,150,150)\r\n",
    "        \r\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\r\n",
    "        #Reduce the image size be factor 2\r\n",
    "        #Shape= (256,12,75,75)\r\n",
    "        \r\n",
    "        \r\n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\r\n",
    "        #Shape= (256,20,75,75)\r\n",
    "        self.relu2=nn.ReLU()\r\n",
    "        #Shape= (256,20,75,75)\r\n",
    "        \r\n",
    "        \r\n",
    "        \r\n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\r\n",
    "        #Shape= (256,32,75,75)\r\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\r\n",
    "        #Shape= (256,32,75,75)\r\n",
    "        self.relu3=nn.ReLU()\r\n",
    "        #Shape= (256,32,75,75)\r\n",
    "        \r\n",
    "        \r\n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\r\n",
    "        \r\n",
    "        \r\n",
    "        \r\n",
    "        #Feed forwad function\r\n",
    "        \r\n",
    "    def forward(self,input):\r\n",
    "        output=self.conv1(input)\r\n",
    "        output=self.bn1(output)\r\n",
    "        output=self.relu1(output)\r\n",
    "            \r\n",
    "        output=self.pool(output)\r\n",
    "            \r\n",
    "        output=self.conv2(output)\r\n",
    "        output=self.relu2(output)\r\n",
    "            \r\n",
    "        output=self.conv3(output)\r\n",
    "        output=self.bn3(output)\r\n",
    "        output=self.relu3(output)\r\n",
    "            \r\n",
    "            \r\n",
    "        #Above output will be in matrix form, with shape (256,32,75,75)\r\n",
    "            \r\n",
    "        output=output.view(-1,32*75*75)\r\n",
    "            \r\n",
    "            \r\n",
    "        output=self.fc(output)\r\n",
    "            \r\n",
    "        return output\r\n",
    "            \r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "## VGG Network\r\n",
    "\r\n",
    "VGG_types = {\r\n",
    "    \"VGGmod\": [32, \"M\", 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 256, 256, \"M\"],   \r\n",
    "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\r\n",
    "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\r\n",
    "    \"VGG16\": [64,64,\"M\",128,128,\"M\",256,256,256,\"M\",512,512,512,\"M\",512,512,512,\"M\"],\r\n",
    "    \"VGG19\": [64,64,\"M\",128,128,\"M\",256,256,256,256,\"M\",512,512,512,512,\"M\",512,512,512,512,\"M\"],\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "class VGG_net(nn.Module):\r\n",
    "    def __init__(self, in_channels, num_classes, type=\"VGG16\"):\r\n",
    "        super(VGG_net, self).__init__()\r\n",
    "        self.in_channels = in_channels\r\n",
    "        self.conv_layers = self.create_conv_layers(VGG_types[type])\r\n",
    "\r\n",
    "        self.fcs = nn.Sequential(\r\n",
    "            nn.Linear(256 * 7 * 7, 4096),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.5),\r\n",
    "            # nn.Linear(4096, 4096),\r\n",
    "            # nn.ReLU(),\r\n",
    "            # nn.Dropout(p=0.5),\r\n",
    "            nn.Linear(4096, num_classes),\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv_layers(x)\r\n",
    "        x = x.reshape(x.shape[0], -1)\r\n",
    "        x = self.fcs(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    def create_conv_layers(self, architecture):\r\n",
    "        layers = []\r\n",
    "        in_channels = self.in_channels\r\n",
    "\r\n",
    "        for x in architecture:\r\n",
    "            if type(x) == int:\r\n",
    "                out_channels = x\r\n",
    "\r\n",
    "                layers += [\r\n",
    "                    nn.Conv2d(\r\n",
    "                        in_channels=in_channels,\r\n",
    "                        out_channels=out_channels,\r\n",
    "                        kernel_size=(3, 3),\r\n",
    "                        stride=(1, 1),\r\n",
    "                        padding=(1, 1),\r\n",
    "                    ),\r\n",
    "                    nn.BatchNorm2d(x),\r\n",
    "                    nn.ReLU(),\r\n",
    "                ]\r\n",
    "                in_channels = x\r\n",
    "            elif x == \"M\":\r\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\r\n",
    "\r\n",
    "        return nn.Sequential(*layers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "## AlexNet implementation\r\n",
    "\r\n",
    "class AlexNet(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Neural network model consisting of layers propsed by AlexNet paper.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, num_classes):\r\n",
    "        \"\"\"\r\n",
    "        Define and allocate layers for this neural net.\r\n",
    "        Args:\r\n",
    "            num_classes (int): number of classes to predict with this model\r\n",
    "        \"\"\"\r\n",
    "        super().__init__()\r\n",
    "        # input size should be : (b x 3 x 227 x 227)\r\n",
    "        # The image in the original paper states that width and height are 224 pixels, but\r\n",
    "        # the dimensions after first convolution layer do not lead to 55 x 55.\r\n",
    "        self.net = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\r\n",
    "            nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\r\n",
    "            nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\r\n",
    "        )\r\n",
    "        # classifier is just a name for linear layers\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Dropout(p=0.5, inplace=True),\r\n",
    "            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.5, inplace=True),\r\n",
    "            nn.Linear(in_features=4096, out_features=4096),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(in_features=4096, out_features=num_classes),\r\n",
    "        )\r\n",
    "        self.init_bias()  # initialize bias\r\n",
    "\r\n",
    "    def init_bias(self):\r\n",
    "        for layer in self.net:\r\n",
    "            if isinstance(layer, nn.Conv2d):\r\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\r\n",
    "                nn.init.constant_(layer.bias, 0)\r\n",
    "        # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\r\n",
    "        nn.init.constant_(self.net[4].bias, 1)\r\n",
    "        nn.init.constant_(self.net[10].bias, 1)\r\n",
    "        nn.init.constant_(self.net[12].bias, 1)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        \"\"\"\r\n",
    "        Pass the input through the net.\r\n",
    "        Args:\r\n",
    "            x (Tensor): input tensor\r\n",
    "        Returns:\r\n",
    "            output (Tensor): output tensor\r\n",
    "        \"\"\"\r\n",
    "        x = self.net(x)\r\n",
    "        x = x.view(-1, 256 * 6 * 6)  # reduce the dimensions for linear layer input\r\n",
    "        return self.classifier(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "## Le Net 5 implementation\r\n",
    "class LeNet(nn.Module):\r\n",
    "    def __init__(self, num_classes):\r\n",
    "        super(LeNet, self).__init__()\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\r\n",
    "        self.conv1 = nn.Conv2d(\r\n",
    "            in_channels=1,\r\n",
    "            out_channels=6,\r\n",
    "            kernel_size=(5, 5),\r\n",
    "            stride=(1, 1),\r\n",
    "            padding=(0, 0),\r\n",
    "        )\r\n",
    "        self.conv2 = nn.Conv2d(\r\n",
    "            in_channels=6,\r\n",
    "            out_channels=16,\r\n",
    "            kernel_size=(5, 5),\r\n",
    "            stride=(1, 1),\r\n",
    "            padding=(0, 0),\r\n",
    "        )\r\n",
    "        self.conv3 = nn.Conv2d(\r\n",
    "            in_channels=16,\r\n",
    "            out_channels=120,\r\n",
    "            kernel_size=(5, 5),\r\n",
    "            stride=(1, 1),\r\n",
    "            padding=(0, 0),\r\n",
    "        )\r\n",
    "        self.linear1 = nn.Linear(120, 84)\r\n",
    "        self.linear2 = nn.Linear(84, num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.relu(self.conv1(x))\r\n",
    "        x = self.pool(x)\r\n",
    "        x = self.relu(self.conv2(x))\r\n",
    "        x = self.pool(x)\r\n",
    "        x = self.relu(\r\n",
    "            self.conv3(x)\r\n",
    "        )  # num_examples x 120 x 1 x 1 --> num_examples x 120\r\n",
    "        x = x.reshape(x.shape[0], -1)\r\n",
    "        x = self.relu(self.linear1(x))\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model=ConvNet(num_classes=len(classes)).to(device)\r\n",
    "\r\n",
    "# model = VGG_net(3, len(classes) , \"VGGmod\").to(device)\r\n",
    "\r\n",
    "# model = MyVGG16(num_classes=len(classes)).to(device)\r\n",
    "\r\n",
    "# model = AlexNet(num_classes=len(classes)).to(device)\r\n",
    "\r\n",
    "# model = LeNet(num_classes=len(classes)).to(device)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#Optmizer and loss function\r\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\r\n",
    "loss_function=nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#calculating the size of training and testing images\r\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\r\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(train_count,test_count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "500 100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#Model training and saving best model\r\n",
    "\r\n",
    "best_accuracy=0.0\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    \r\n",
    "    #Evaluation and training on training dataset\r\n",
    "    model.train()\r\n",
    "    train_accuracy=0.0\r\n",
    "    train_loss=0.0\r\n",
    "    \r\n",
    "    for i, (images,labels) in enumerate(train_loader):\r\n",
    "        if torch.cuda.is_available():\r\n",
    "            images=Variable(images.cuda())\r\n",
    "            labels=Variable(labels.cuda())\r\n",
    "            \r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        outputs=model(images)\r\n",
    "        loss=loss_function(outputs,labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        \r\n",
    "        train_loss+= loss.cpu().data*images.size(0)\r\n",
    "        _,prediction=torch.max(outputs.data,1)\r\n",
    "        \r\n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\r\n",
    "        \r\n",
    "    train_accuracy=train_accuracy/train_count\r\n",
    "    train_loss=train_loss/train_count\r\n",
    "    \r\n",
    "    \r\n",
    "    # Evaluation on testing dataset\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    test_accuracy=0.0\r\n",
    "    for i, (images,labels) in enumerate(test_loader):\r\n",
    "        if torch.cuda.is_available():\r\n",
    "            images=Variable(images.cuda())\r\n",
    "            labels=Variable(labels.cuda())\r\n",
    "            \r\n",
    "        outputs=model(images)\r\n",
    "        _,prediction=torch.max(outputs.data,1)\r\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\r\n",
    "    \r\n",
    "    test_accuracy=test_accuracy/test_count\r\n",
    "    \r\n",
    "    \r\n",
    "    print('Epoch: '+str(epoch+1)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\r\n",
    "    \r\n",
    "    #Save the best model\r\n",
    "    if test_accuracy>=best_accuracy:\r\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\r\n",
    "        best_accuracy=test_accuracy\r\n",
    "    \r\n",
    "       \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Vinit\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1 Train Loss: tensor(29.5906) Train Accuracy: 0.404 Test Accuracy: 0.67\n",
      "Epoch: 2 Train Loss: tensor(7.5503) Train Accuracy: 0.77 Test Accuracy: 0.91\n",
      "Epoch: 3 Train Loss: tensor(5.2636) Train Accuracy: 0.806 Test Accuracy: 0.93\n",
      "Epoch: 4 Train Loss: tensor(3.0753) Train Accuracy: 0.87 Test Accuracy: 0.84\n",
      "Epoch: 5 Train Loss: tensor(2.6080) Train Accuracy: 0.862 Test Accuracy: 0.92\n",
      "Epoch: 6 Train Loss: tensor(1.4513) Train Accuracy: 0.926 Test Accuracy: 0.97\n",
      "Epoch: 7 Train Loss: tensor(0.9111) Train Accuracy: 0.954 Test Accuracy: 0.98\n",
      "Epoch: 8 Train Loss: tensor(0.6107) Train Accuracy: 0.948 Test Accuracy: 0.95\n",
      "Epoch: 9 Train Loss: tensor(0.2660) Train Accuracy: 0.972 Test Accuracy: 1.0\n",
      "Epoch: 10 Train Loss: tensor(1.0361) Train Accuracy: 0.942 Test Accuracy: 0.92\n",
      "Epoch: 11 Train Loss: tensor(0.6932) Train Accuracy: 0.944 Test Accuracy: 0.95\n",
      "Epoch: 12 Train Loss: tensor(1.3590) Train Accuracy: 0.944 Test Accuracy: 0.92\n",
      "Epoch: 13 Train Loss: tensor(0.3381) Train Accuracy: 0.968 Test Accuracy: 0.97\n",
      "Epoch: 14 Train Loss: tensor(0.5824) Train Accuracy: 0.954 Test Accuracy: 0.92\n",
      "Epoch: 15 Train Loss: tensor(0.5110) Train Accuracy: 0.964 Test Accuracy: 0.99\n",
      "Epoch: 16 Train Loss: tensor(0.1861) Train Accuracy: 0.978 Test Accuracy: 0.99\n",
      "Epoch: 17 Train Loss: tensor(0.6323) Train Accuracy: 0.95 Test Accuracy: 0.99\n",
      "Epoch: 18 Train Loss: tensor(0.4083) Train Accuracy: 0.976 Test Accuracy: 0.99\n",
      "Epoch: 19 Train Loss: tensor(0.1026) Train Accuracy: 0.986 Test Accuracy: 1.0\n",
      "Epoch: 20 Train Loss: tensor(0.0526) Train Accuracy: 0.994 Test Accuracy: 0.97\n",
      "Epoch: 21 Train Loss: tensor(0.1855) Train Accuracy: 0.982 Test Accuracy: 0.96\n",
      "Epoch: 22 Train Loss: tensor(0.6245) Train Accuracy: 0.948 Test Accuracy: 0.99\n",
      "Epoch: 23 Train Loss: tensor(0.4897) Train Accuracy: 0.962 Test Accuracy: 1.0\n",
      "Epoch: 24 Train Loss: tensor(0.0984) Train Accuracy: 0.98 Test Accuracy: 0.99\n",
      "Epoch: 25 Train Loss: tensor(0.1753) Train Accuracy: 0.976 Test Accuracy: 0.93\n",
      "Epoch: 26 Train Loss: tensor(0.0603) Train Accuracy: 0.986 Test Accuracy: 1.0\n",
      "Epoch: 27 Train Loss: tensor(0.0793) Train Accuracy: 0.986 Test Accuracy: 1.0\n",
      "Epoch: 28 Train Loss: tensor(0.0297) Train Accuracy: 0.992 Test Accuracy: 1.0\n",
      "Epoch: 29 Train Loss: tensor(0.0602) Train Accuracy: 0.984 Test Accuracy: 0.99\n",
      "Epoch: 30 Train Loss: tensor(0.1406) Train Accuracy: 0.988 Test Accuracy: 1.0\n",
      "Epoch: 31 Train Loss: tensor(0.1293) Train Accuracy: 0.978 Test Accuracy: 0.99\n",
      "Epoch: 32 Train Loss: tensor(0.3208) Train Accuracy: 0.96 Test Accuracy: 0.99\n",
      "Epoch: 33 Train Loss: tensor(0.2560) Train Accuracy: 0.978 Test Accuracy: 0.96\n",
      "Epoch: 34 Train Loss: tensor(0.3955) Train Accuracy: 0.95 Test Accuracy: 0.99\n",
      "Epoch: 35 Train Loss: tensor(0.0127) Train Accuracy: 0.994 Test Accuracy: 1.0\n",
      "Epoch: 36 Train Loss: tensor(0.0075) Train Accuracy: 0.996 Test Accuracy: 1.0\n",
      "Epoch: 37 Train Loss: tensor(0.0013) Train Accuracy: 1.0 Test Accuracy: 1.0\n",
      "Epoch: 38 Train Loss: tensor(0.0008) Train Accuracy: 1.0 Test Accuracy: 1.0\n",
      "Epoch: 39 Train Loss: tensor(0.0008) Train Accuracy: 1.0 Test Accuracy: 1.0\n",
      "Epoch: 40 Train Loss: tensor(0.0247) Train Accuracy: 0.994 Test Accuracy: 0.98\n",
      "Epoch: 41 Train Loss: tensor(0.1638) Train Accuracy: 0.982 Test Accuracy: 1.0\n",
      "Epoch: 42 Train Loss: tensor(0.1832) Train Accuracy: 0.98 Test Accuracy: 0.99\n",
      "Epoch: 43 Train Loss: tensor(0.1029) Train Accuracy: 0.982 Test Accuracy: 0.94\n",
      "Epoch: 44 Train Loss: tensor(0.1711) Train Accuracy: 0.982 Test Accuracy: 1.0\n",
      "Epoch: 45 Train Loss: tensor(0.1451) Train Accuracy: 0.984 Test Accuracy: 0.93\n",
      "Epoch: 46 Train Loss: tensor(0.2601) Train Accuracy: 0.968 Test Accuracy: 0.98\n",
      "Epoch: 47 Train Loss: tensor(0.0434) Train Accuracy: 0.988 Test Accuracy: 1.0\n",
      "Epoch: 48 Train Loss: tensor(0.0126) Train Accuracy: 0.998 Test Accuracy: 0.99\n",
      "Epoch: 49 Train Loss: tensor(0.0106) Train Accuracy: 0.996 Test Accuracy: 0.98\n",
      "Epoch: 50 Train Loss: tensor(0.0013) Train Accuracy: 1.0 Test Accuracy: 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "checkpoint = torch.load('best_checkpoint.model')\r\n",
    "model.load_state_dict(checkpoint)\r\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc): Linear(in_features=180000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#Transforms\r\n",
    "transformer=transforms.Compose([\r\n",
    "    transforms.Resize((150,150)),\r\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\r\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\r\n",
    "                        [0.5,0.5,0.5])\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "## Making predictions\r\n",
    "from PIL import Image\r\n",
    "from io import open\r\n",
    "\r\n",
    "\r\n",
    "def prediction(path, transform):\r\n",
    "    image=Image.open(path)\r\n",
    "    image_tensor=transformer(image).float()\r\n",
    "    image_tensor=image_tensor.unsqueeze_(0)\r\n",
    "    image_tensor=image_tensor.to(device)\r\n",
    "    output=model(image_tensor)\r\n",
    "    _,prediction=torch.max(output.data,1)\r\n",
    "    return classes[prediction[0]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "test_path = \"Dataset\\\\test\"\r\n",
    "image_path=glob.glob(test_path+'/*.jpg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "predictions={}\r\n",
    "for i,path in enumerate(image_path):\r\n",
    "    predictions[path[len(test_path)+1:]]=prediction(path,transformer)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'101.jpg': 'overpass',\n",
       " '102.jpg': 'runway',\n",
       " '103.jpg': 'railway',\n",
       " '104.jpg': 'oil_well',\n",
       " '105.jpg': 'bridge',\n",
       " '106.jpg': 'railway',\n",
       " '107.jpg': 'basketball_court',\n",
       " '108.jpg': 'golf_course',\n",
       " '109.jpg': 'golf_course',\n",
       " '110.jpg': 'overpass',\n",
       " '111.jpg': 'bridge',\n",
       " '112.jpg': 'basketball_court',\n",
       " '113.jpg': 'oil_well',\n",
       " '114.jpg': 'overpass',\n",
       " '115.jpg': 'runway',\n",
       " '116.jpg': 'swimming_pool',\n",
       " '117.jpg': 'basketball_court',\n",
       " '118.jpg': 'oil_well',\n",
       " '119.jpg': 'runway',\n",
       " '120.jpg': 'basketball_court',\n",
       " '121.jpg': 'oil_well',\n",
       " '122.jpg': 'bridge',\n",
       " '123.jpg': 'golf_course',\n",
       " '124.jpg': 'tennis_court',\n",
       " '125.jpg': 'crosswalk',\n",
       " '126.jpg': 'basketball_court',\n",
       " '127.jpg': 'bridge',\n",
       " '128.jpg': 'overpass',\n",
       " '129.jpg': 'oil_well',\n",
       " '130.jpg': 'crosswalk',\n",
       " '131.jpg': 'golf_course',\n",
       " '132.jpg': 'railway',\n",
       " '133.jpg': 'basketball_court',\n",
       " '134.jpg': 'runway',\n",
       " '135.jpg': 'overpass',\n",
       " '136.jpg': 'golf_course',\n",
       " '137.jpg': 'golf_course',\n",
       " '138.jpg': 'bridge',\n",
       " '139.jpg': 'overpass',\n",
       " '140.jpg': 'golf_course',\n",
       " '141.jpg': 'basketball_court',\n",
       " '142.jpg': 'runway',\n",
       " '143.jpg': 'railway',\n",
       " '144.jpg': 'bridge',\n",
       " '145.jpg': 'basketball_court',\n",
       " '146.jpg': 'overpass',\n",
       " '147.jpg': 'overpass',\n",
       " '148.jpg': 'oil_well',\n",
       " '149.jpg': 'crosswalk',\n",
       " '150.jpg': 'bridge',\n",
       " '151.jpg': 'runway',\n",
       " '152.jpg': 'overpass',\n",
       " '153.jpg': 'tennis_court',\n",
       " '154.jpg': 'overpass',\n",
       " '155.jpg': 'railway',\n",
       " '156.jpg': 'basketball_court',\n",
       " '157.jpg': 'basketball_court',\n",
       " '158.jpg': 'crosswalk',\n",
       " '159.jpg': 'runway',\n",
       " '160.jpg': 'bridge',\n",
       " '161.jpg': 'runway',\n",
       " '162.jpg': 'overpass',\n",
       " '163.jpg': 'golf_course',\n",
       " '164.jpg': 'bridge',\n",
       " '165.jpg': 'bridge',\n",
       " '166.jpg': 'golf_course',\n",
       " '167.jpg': 'overpass',\n",
       " '168.jpg': 'golf_course',\n",
       " '169.jpg': 'golf_course',\n",
       " '170.jpg': 'tennis_court',\n",
       " '171.jpg': 'overpass',\n",
       " '172.jpg': 'overpass',\n",
       " '173.jpg': 'swimming_pool',\n",
       " '174.jpg': 'golf_course',\n",
       " '175.jpg': 'runway',\n",
       " '176.jpg': 'crosswalk',\n",
       " '177.jpg': 'overpass',\n",
       " '178.jpg': 'tennis_court',\n",
       " '179.jpg': 'overpass',\n",
       " '180.jpg': 'railway',\n",
       " '181.jpg': 'bridge',\n",
       " '182.jpg': 'overpass',\n",
       " '183.jpg': 'bridge',\n",
       " '184.jpg': 'runway',\n",
       " '185.jpg': 'crosswalk',\n",
       " '186.jpg': 'tennis_court',\n",
       " '187.jpg': 'runway',\n",
       " '188.jpg': 'basketball_court',\n",
       " '189.jpg': 'oil_well',\n",
       " '190.jpg': 'overpass',\n",
       " '191.jpg': 'swimming_pool',\n",
       " '192.jpg': 'golf_course',\n",
       " '193.jpg': 'overpass',\n",
       " '194.jpg': 'overpass',\n",
       " '195.jpg': 'bridge',\n",
       " '196.jpg': 'basketball_court',\n",
       " '197.jpg': 'oil_well',\n",
       " '198.jpg': 'crosswalk',\n",
       " '199.jpg': 'overpass',\n",
       " '200.jpg': 'runway'}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "label_dict = {\r\n",
    "    \"basketball_court\": 1, \r\n",
    "    \"bridge\":2, \r\n",
    "    \"crosswalk\":3, \r\n",
    "    \"golf_course\":4, \r\n",
    "    \"oil_well\":5, \r\n",
    "    \"overpass\":6, \r\n",
    "    \"railway\":7, \r\n",
    "    \"runway\":8, \r\n",
    "    \"swimming_pool\":9, \r\n",
    "    \"tennis_court\":10\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "## Convert the predictions to labels from label_dict\r\n",
    "predictions_labels={}\r\n",
    "for key,value in predictions.items():\r\n",
    "    predictions_labels[key]=label_dict[value]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "predictions_labels"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'101.jpg': 6,\n",
       " '102.jpg': 8,\n",
       " '103.jpg': 7,\n",
       " '104.jpg': 5,\n",
       " '105.jpg': 2,\n",
       " '106.jpg': 7,\n",
       " '107.jpg': 1,\n",
       " '108.jpg': 4,\n",
       " '109.jpg': 4,\n",
       " '110.jpg': 6,\n",
       " '111.jpg': 2,\n",
       " '112.jpg': 1,\n",
       " '113.jpg': 5,\n",
       " '114.jpg': 6,\n",
       " '115.jpg': 8,\n",
       " '116.jpg': 9,\n",
       " '117.jpg': 1,\n",
       " '118.jpg': 5,\n",
       " '119.jpg': 8,\n",
       " '120.jpg': 1,\n",
       " '121.jpg': 5,\n",
       " '122.jpg': 2,\n",
       " '123.jpg': 4,\n",
       " '124.jpg': 10,\n",
       " '125.jpg': 3,\n",
       " '126.jpg': 1,\n",
       " '127.jpg': 2,\n",
       " '128.jpg': 6,\n",
       " '129.jpg': 5,\n",
       " '130.jpg': 3,\n",
       " '131.jpg': 4,\n",
       " '132.jpg': 7,\n",
       " '133.jpg': 1,\n",
       " '134.jpg': 8,\n",
       " '135.jpg': 6,\n",
       " '136.jpg': 4,\n",
       " '137.jpg': 4,\n",
       " '138.jpg': 2,\n",
       " '139.jpg': 6,\n",
       " '140.jpg': 4,\n",
       " '141.jpg': 1,\n",
       " '142.jpg': 8,\n",
       " '143.jpg': 7,\n",
       " '144.jpg': 2,\n",
       " '145.jpg': 1,\n",
       " '146.jpg': 6,\n",
       " '147.jpg': 6,\n",
       " '148.jpg': 5,\n",
       " '149.jpg': 3,\n",
       " '150.jpg': 2,\n",
       " '151.jpg': 8,\n",
       " '152.jpg': 6,\n",
       " '153.jpg': 10,\n",
       " '154.jpg': 6,\n",
       " '155.jpg': 7,\n",
       " '156.jpg': 1,\n",
       " '157.jpg': 1,\n",
       " '158.jpg': 3,\n",
       " '159.jpg': 8,\n",
       " '160.jpg': 2,\n",
       " '161.jpg': 8,\n",
       " '162.jpg': 6,\n",
       " '163.jpg': 4,\n",
       " '164.jpg': 2,\n",
       " '165.jpg': 2,\n",
       " '166.jpg': 4,\n",
       " '167.jpg': 6,\n",
       " '168.jpg': 4,\n",
       " '169.jpg': 4,\n",
       " '170.jpg': 10,\n",
       " '171.jpg': 6,\n",
       " '172.jpg': 6,\n",
       " '173.jpg': 9,\n",
       " '174.jpg': 4,\n",
       " '175.jpg': 8,\n",
       " '176.jpg': 3,\n",
       " '177.jpg': 6,\n",
       " '178.jpg': 10,\n",
       " '179.jpg': 6,\n",
       " '180.jpg': 7,\n",
       " '181.jpg': 2,\n",
       " '182.jpg': 6,\n",
       " '183.jpg': 2,\n",
       " '184.jpg': 8,\n",
       " '185.jpg': 3,\n",
       " '186.jpg': 10,\n",
       " '187.jpg': 8,\n",
       " '188.jpg': 1,\n",
       " '189.jpg': 5,\n",
       " '190.jpg': 6,\n",
       " '191.jpg': 9,\n",
       " '192.jpg': 4,\n",
       " '193.jpg': 6,\n",
       " '194.jpg': 6,\n",
       " '195.jpg': 2,\n",
       " '196.jpg': 1,\n",
       " '197.jpg': 5,\n",
       " '198.jpg': 3,\n",
       " '199.jpg': 6,\n",
       " '200.jpg': 8}"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting the predictions to CSV format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Convert predictions to dataframe\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Make a dataframe with the predictions with column names as ImageID and LabelID\r\n",
    "predictions_df = pd.DataFrame.from_dict(predictions_labels, orient='index')\r\n",
    "\r\n",
    "# Removing the .jpg from the file names\r\n",
    "predictions_df.index=predictions_df.index.str.replace('.jpg','')\r\n",
    "\r\n",
    "# Naming the columns as ImageID and Label\r\n",
    "predictions_df.reset_index(level=0, inplace=True)\r\n",
    "predictions_df.columns = ['ImageID', 'LabelID']\r\n",
    "\r\n",
    "# Removing the index from the dataframe\r\n",
    "predictions_df.reset_index(drop= True, inplace=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Vinit\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "predictions_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>LabelID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>197</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>198</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>199</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageID  LabelID\n",
       "0      101        6\n",
       "1      102        8\n",
       "2      103        7\n",
       "3      104        5\n",
       "4      105        2\n",
       "..     ...      ...\n",
       "95     196        1\n",
       "96     197        5\n",
       "97     198        3\n",
       "98     199        6\n",
       "99     200        8\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "## Saving the predictions to csv\r\n",
    "predictions_df.to_csv('18D070067.csv',index=False)   #18D070067.csv is the name of the csv file and the index have been dropped"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '18D070067.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41612/1290782912.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Saving the predictions to csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'18D070067.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#18D070067.csv is the name of the csv file and the index have been dropped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3477\u001b[0m             \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m             \u001b[0mescapechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3479\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3480\u001b[0m         )\n\u001b[0;32m   3481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m         ) as handles:\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '18D070067.csv'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hence, the implementation of the model is complete and the predictions are converted to CSV format."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('gpu-pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "3d1484f6a5966aacf1d005944f88794f1e4469136ed9878e517c1d6594061a94"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}