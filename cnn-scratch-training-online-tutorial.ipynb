{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Load libraries\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import glob\r\n",
    "import torch.nn as nn\r\n",
    "from torchvision.transforms import transforms\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.optim import Adam\r\n",
    "from torch.autograd import Variable\r\n",
    "import torchvision\r\n",
    "import pathlib"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#checking for device\r\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "## Hyperparameters\r\n",
    "batch_size_ = 2\r\n",
    "num_epochs = 20\r\n",
    "size = 256"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Transforms\r\n",
    "transformer=transforms.Compose([\r\n",
    "    transforms.Resize((size,size)),\r\n",
    "    transforms.RandomHorizontalFlip(),\r\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\r\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\r\n",
    "                        [0.5,0.5,0.5])\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Dataloader\r\n",
    "\r\n",
    "#Path for training and testing directory\r\n",
    "train_path='Dataset\\\\train'\r\n",
    "test_path='Dataset\\\\val'\r\n",
    "\r\n",
    "train_loader=DataLoader(\r\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\r\n",
    "    batch_size=batch_size_, shuffle=True\r\n",
    ")\r\n",
    "test_loader=DataLoader(\r\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\r\n",
    "    batch_size=batch_size_, shuffle=True\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#categories\r\n",
    "root=pathlib.Path(train_path)\r\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(classes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['basketball_court', 'bridge', 'crosswalk', 'golf_course', 'oil_well', 'overpass', 'railway', 'runway', 'swimming_pool', 'tennis_court']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#CNN Network\r\n",
    "\r\n",
    "\r\n",
    "class ConvNet(nn.Module):\r\n",
    "    def __init__(self,num_classes):\r\n",
    "        super(ConvNet,self).__init__()\r\n",
    "        \r\n",
    "        #Output size after convolution filter\r\n",
    "        #((w-f+2P)/s) +1\r\n",
    "        \r\n",
    "        #Input shape= (256,3,256,256)\r\n",
    "        \r\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\r\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\r\n",
    "        self.relu1=nn.ReLU()\r\n",
    "        \r\n",
    "        self.pool1=nn.MaxPool2d(kernel_size=2)\r\n",
    "        \r\n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\r\n",
    "        self.relu2=nn.ReLU()\r\n",
    "        \r\n",
    "        \r\n",
    "        \r\n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\r\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\r\n",
    "        self.relu3=nn.ReLU()\r\n",
    "        #Shape= (batch_size,32,128,128)\r\n",
    "\r\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\r\n",
    "\r\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\r\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=64)\r\n",
    "        self.relu4 = nn.ReLU()\r\n",
    "        # Shape = (batch_size,64,64,64)\r\n",
    "\r\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\r\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=128)\r\n",
    "        self.relu5 = nn.ReLU()\r\n",
    "        # Shape = (batch_size,128,64,64)\r\n",
    "\r\n",
    "        \r\n",
    "        self.fc=nn.Linear(in_features=128*64*64 ,out_features=num_classes)\r\n",
    "        \r\n",
    "        \r\n",
    "        \r\n",
    "        #Feed forwad function\r\n",
    "        \r\n",
    "    def forward(self,input):\r\n",
    "        output=self.conv1(input)\r\n",
    "        output=self.bn1(output)\r\n",
    "        output=self.relu1(output)\r\n",
    "            \r\n",
    "        output=self.pool1(output)\r\n",
    "            \r\n",
    "        output=self.conv2(output)\r\n",
    "        output=self.relu2(output)\r\n",
    "            \r\n",
    "        output=self.conv3(output)\r\n",
    "        output=self.bn3(output)\r\n",
    "        output=self.relu3(output)\r\n",
    "        \r\n",
    "        output=self.pool2(output)\r\n",
    "\r\n",
    "        output=self.conv4(output)\r\n",
    "        output=self.bn4(output)\r\n",
    "        output=self.relu4(output)\r\n",
    "\r\n",
    "        output=self.conv5(output)\r\n",
    "        output=self.bn5(output)\r\n",
    "        output=self.relu5(output)\r\n",
    "\r\n",
    "            \r\n",
    "        #Above output will be in matrix form, with shape (256,128,64,64)\r\n",
    "        \r\n",
    "        output=output.view(-1,128*64*64)\r\n",
    "            \r\n",
    "            \r\n",
    "        output=self.fc(output)\r\n",
    "            \r\n",
    "        return output\r\n",
    "            \r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "## VGG Network\r\n",
    "\r\n",
    "VGG_types = {\r\n",
    "    \"MyConv\": [16,\"M\",32,32,\"M\",64,64,\"M\",128,128,\"M\"],\r\n",
    "    \"VGGmod\":[16, \"M\", 32, \"M\", 64,64, \"M\", 128,128, \"M\"], #256, 256, \"M\"],   \r\n",
    "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\r\n",
    "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\r\n",
    "    \"VGG16\": [64,64,\"M\",128,128,\"M\",256,256,256,\"M\",512,512,512,\"M\",512,512,512,\"M\"],\r\n",
    "    \"VGG19\": [64,64,\"M\",128,128,\"M\",256,256,256,256,\"M\",512,512,512,512,\"M\",512,512,512,512,\"M\"],\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "class VGG_net(nn.Module):\r\n",
    "    def __init__(self, in_channels, num_classes, type=\"VGG16\"):\r\n",
    "        super(VGG_net, self).__init__()\r\n",
    "        self.in_channels = in_channels\r\n",
    "        self.conv_layers = self.create_conv_layers(VGG_types[type])\r\n",
    "\r\n",
    "        self.fcs = nn.Sequential(\r\n",
    "            nn.Linear(128 * 16 * 16, 6144),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.5),\r\n",
    "            # nn.Linear(4096, 4096),\r\n",
    "            # nn.ReLU(),\r\n",
    "            # nn.Dropout(p=0.5),\r\n",
    "            nn.Linear(6144, num_classes),\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv_layers(x)\r\n",
    "        x = x.reshape(x.shape[0], -1)\r\n",
    "        x = self.fcs(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    def create_conv_layers(self, architecture):\r\n",
    "        layers = []\r\n",
    "        in_channels = self.in_channels\r\n",
    "\r\n",
    "        for x in architecture:\r\n",
    "            if type(x) == int:\r\n",
    "                out_channels = x\r\n",
    "\r\n",
    "                layers += [\r\n",
    "                    nn.Conv2d(\r\n",
    "                        in_channels=in_channels,\r\n",
    "                        out_channels=out_channels,\r\n",
    "                        kernel_size=(3, 3),\r\n",
    "                        stride=(1, 1),\r\n",
    "                        padding=(1, 1),\r\n",
    "                    ),\r\n",
    "                    nn.BatchNorm2d(x),\r\n",
    "                    nn.ReLU(),\r\n",
    "                ]\r\n",
    "                in_channels = x\r\n",
    "            elif x == \"M\":\r\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\r\n",
    "\r\n",
    "        return nn.Sequential(*layers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "## AlexNet implementation\r\n",
    "\r\n",
    "class AlexNet(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Neural network model consisting of layers propsed by AlexNet paper.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, num_classes):\r\n",
    "        \"\"\"\r\n",
    "        Define and allocate layers for this neural net.\r\n",
    "        Args:\r\n",
    "            num_classes (int): number of classes to predict with this model\r\n",
    "        \"\"\"\r\n",
    "        super().__init__()\r\n",
    "        # input size should be : (b x 3 x 227 x 227)\r\n",
    "        # The image in the original paper states that width and height are 224 pixels, but\r\n",
    "        # the dimensions after first convolution layer do not lead to 55 x 55.\r\n",
    "        self.net = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\r\n",
    "            nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\r\n",
    "            nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\r\n",
    "        )\r\n",
    "        # classifier is just a name for linear layers\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Dropout(p=0.5, inplace=True),\r\n",
    "            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.5, inplace=True),\r\n",
    "            nn.Linear(in_features=4096, out_features=4096),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(in_features=4096, out_features=num_classes),\r\n",
    "        )\r\n",
    "        self.init_bias()  # initialize bias\r\n",
    "\r\n",
    "    def init_bias(self):\r\n",
    "        for layer in self.net:\r\n",
    "            if isinstance(layer, nn.Conv2d):\r\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.01)\r\n",
    "                nn.init.constant_(layer.bias, 0)\r\n",
    "        # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\r\n",
    "        nn.init.constant_(self.net[4].bias, 1)\r\n",
    "        nn.init.constant_(self.net[10].bias, 1)\r\n",
    "        nn.init.constant_(self.net[12].bias, 1)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        \"\"\"\r\n",
    "        Pass the input through the net.\r\n",
    "        Args:\r\n",
    "            x (Tensor): input tensor\r\n",
    "        Returns:\r\n",
    "            output (Tensor): output tensor\r\n",
    "        \"\"\"\r\n",
    "        x = self.net(x)\r\n",
    "        x = x.view(-1, 256 * 6 * 6)  # reduce the dimensions for linear layer input\r\n",
    "        return self.classifier(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "## Le Net 5 implementation\r\n",
    "class LeNet(nn.Module):\r\n",
    "    def __init__(self, num_classes):\r\n",
    "        super(LeNet, self).__init__()\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\r\n",
    "        self.conv1 = nn.Conv2d(\r\n",
    "            in_channels=1,\r\n",
    "            out_channels=6,\r\n",
    "            kernel_size=(5, 5),\r\n",
    "            stride=(1, 1),\r\n",
    "            padding=(0, 0),\r\n",
    "        )\r\n",
    "        self.conv2 = nn.Conv2d(\r\n",
    "            in_channels=6,\r\n",
    "            out_channels=16,\r\n",
    "            kernel_size=(5, 5),\r\n",
    "            stride=(1, 1),\r\n",
    "            padding=(0, 0),\r\n",
    "        )\r\n",
    "        self.conv3 = nn.Conv2d(\r\n",
    "            in_channels=16,\r\n",
    "            out_channels=120,\r\n",
    "            kernel_size=(5, 5),\r\n",
    "            stride=(1, 1),\r\n",
    "            padding=(0, 0),\r\n",
    "        )\r\n",
    "        self.linear1 = nn.Linear(120, 84)\r\n",
    "        self.linear2 = nn.Linear(84, num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.relu(self.conv1(x))\r\n",
    "        x = self.pool(x)\r\n",
    "        x = self.relu(self.conv2(x))\r\n",
    "        x = self.pool(x)\r\n",
    "        x = self.relu(\r\n",
    "            self.conv3(x)\r\n",
    "        )  # num_examples x 120 x 1 x 1 --> num_examples x 120\r\n",
    "        x = x.reshape(x.shape[0], -1)\r\n",
    "        x = self.relu(self.linear1(x))\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# model=ConvNet(num_classes=len(classes)).to(device)\r\n",
    "\r\n",
    "model = VGG_net(3, len(classes) , \"MyConv\").to(device)\r\n",
    "\r\n",
    "# model = MyVGG16(num_classes=len(classes)).to(device)\r\n",
    "\r\n",
    "# model = AlexNet(num_classes=len(classes)).to(device)\r\n",
    "\r\n",
    "# model = LeNet(num_classes=len(classes)).to(device)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#Optmizer and loss function\r\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\r\n",
    "loss_function=nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#calculating the size of training and testing images\r\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\r\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(train_count,test_count)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "500 100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#Model training and saving best model\r\n",
    "\r\n",
    "best_accuracy=0.0\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    \r\n",
    "    #Evaluation and training on training dataset\r\n",
    "    model.train()\r\n",
    "    train_accuracy=0.0\r\n",
    "    train_loss=0.0\r\n",
    "    \r\n",
    "    for i, (images,labels) in enumerate(train_loader):\r\n",
    "        if torch.cuda.is_available():\r\n",
    "            images=Variable(images.cuda())\r\n",
    "            labels=Variable(labels.cuda())\r\n",
    "            \r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        outputs=model(images)\r\n",
    "        loss=loss_function(outputs,labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        \r\n",
    "        train_loss+= loss.cpu().data*images.size(0)\r\n",
    "        _,prediction=torch.max(outputs.data,1)\r\n",
    "        \r\n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\r\n",
    "        \r\n",
    "    train_accuracy=train_accuracy/train_count\r\n",
    "    train_loss=train_loss/train_count\r\n",
    "    \r\n",
    "    \r\n",
    "    # Evaluation on testing dataset\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    test_accuracy=0.0\r\n",
    "    for i, (images,labels) in enumerate(test_loader):\r\n",
    "        if torch.cuda.is_available():\r\n",
    "            images=Variable(images.cuda())\r\n",
    "            labels=Variable(labels.cuda())\r\n",
    "            \r\n",
    "        outputs=model(images)\r\n",
    "        _,prediction=torch.max(outputs.data,1)\r\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\r\n",
    "    \r\n",
    "    test_accuracy=test_accuracy/test_count\r\n",
    "    \r\n",
    "    \r\n",
    "    print('Epoch: '+str(epoch+1)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\r\n",
    "    \r\n",
    "    #Save the best model\r\n",
    "    if test_accuracy>=best_accuracy:\r\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\r\n",
    "        best_accuracy=test_accuracy\r\n",
    "    \r\n",
    "       \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Vinit\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 640.00 MiB (GPU 0; 6.00 GiB total capacity; 3.76 GiB already allocated; 0 bytes free; 3.78 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31248/3632710154.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 640.00 MiB (GPU 0; 6.00 GiB total capacity; 3.76 GiB already allocated; 0 bytes free; 3.78 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "checkpoint = torch.load('best_checkpoint.model')\r\n",
    "model.load_state_dict(checkpoint)\r\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VGG_net(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU()\n",
       "    (17): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (18): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU()\n",
       "    (21): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU()\n",
       "    (24): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fcs): Sequential(\n",
       "    (0): Linear(in_features=32768, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Transforms\r\n",
    "transformer=transforms.Compose([\r\n",
    "    transforms.Resize((size,size)),\r\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\r\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\r\n",
    "                        [0.5,0.5,0.5])\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Making predictions\r\n",
    "from PIL import Image\r\n",
    "from io import open\r\n",
    "\r\n",
    "\r\n",
    "def prediction(path, transform):\r\n",
    "    image=Image.open(path)\r\n",
    "    image_tensor=transformer(image).float()\r\n",
    "    image_tensor=image_tensor.unsqueeze_(0)\r\n",
    "    image_tensor=image_tensor.to(device)\r\n",
    "    output=model(image_tensor)\r\n",
    "    _,prediction=torch.max(output.data,1)\r\n",
    "    return classes[prediction[0]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_path = \"Dataset\\\\test\"\r\n",
    "image_path=glob.glob(test_path+'/*.jpg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions={}\r\n",
    "for i,path in enumerate(image_path):\r\n",
    "    predictions[path[len(test_path)+1:]]=prediction(path,transformer)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'101.jpg': 'oil_well',\n",
       " '102.jpg': 'railway',\n",
       " '103.jpg': 'crosswalk',\n",
       " '104.jpg': 'oil_well',\n",
       " '105.jpg': 'overpass',\n",
       " '106.jpg': 'bridge',\n",
       " '107.jpg': 'runway',\n",
       " '108.jpg': 'oil_well',\n",
       " '109.jpg': 'overpass',\n",
       " '110.jpg': 'overpass',\n",
       " '111.jpg': 'bridge',\n",
       " '112.jpg': 'tennis_court',\n",
       " '113.jpg': 'oil_well',\n",
       " '114.jpg': 'overpass',\n",
       " '115.jpg': 'runway',\n",
       " '116.jpg': 'swimming_pool',\n",
       " '117.jpg': 'overpass',\n",
       " '118.jpg': 'runway',\n",
       " '119.jpg': 'runway',\n",
       " '120.jpg': 'railway',\n",
       " '121.jpg': 'oil_well',\n",
       " '122.jpg': 'overpass',\n",
       " '123.jpg': 'overpass',\n",
       " '124.jpg': 'bridge',\n",
       " '125.jpg': 'crosswalk',\n",
       " '126.jpg': 'bridge',\n",
       " '127.jpg': 'bridge',\n",
       " '128.jpg': 'basketball_court',\n",
       " '129.jpg': 'oil_well',\n",
       " '130.jpg': 'runway',\n",
       " '131.jpg': 'bridge',\n",
       " '132.jpg': 'basketball_court',\n",
       " '133.jpg': 'crosswalk',\n",
       " '134.jpg': 'crosswalk',\n",
       " '135.jpg': 'overpass',\n",
       " '136.jpg': 'bridge',\n",
       " '137.jpg': 'overpass',\n",
       " '138.jpg': 'overpass',\n",
       " '139.jpg': 'railway',\n",
       " '140.jpg': 'bridge',\n",
       " '141.jpg': 'railway',\n",
       " '142.jpg': 'railway',\n",
       " '143.jpg': 'bridge',\n",
       " '144.jpg': 'bridge',\n",
       " '145.jpg': 'overpass',\n",
       " '146.jpg': 'bridge',\n",
       " '147.jpg': 'tennis_court',\n",
       " '148.jpg': 'oil_well',\n",
       " '149.jpg': 'basketball_court',\n",
       " '150.jpg': 'bridge',\n",
       " '151.jpg': 'runway',\n",
       " '152.jpg': 'overpass',\n",
       " '153.jpg': 'basketball_court',\n",
       " '154.jpg': 'overpass',\n",
       " '155.jpg': 'basketball_court',\n",
       " '156.jpg': 'bridge',\n",
       " '157.jpg': 'runway',\n",
       " '158.jpg': 'crosswalk',\n",
       " '159.jpg': 'overpass',\n",
       " '160.jpg': 'overpass',\n",
       " '161.jpg': 'basketball_court',\n",
       " '162.jpg': 'overpass',\n",
       " '163.jpg': 'bridge',\n",
       " '164.jpg': 'overpass',\n",
       " '165.jpg': 'bridge',\n",
       " '166.jpg': 'bridge',\n",
       " '167.jpg': 'overpass',\n",
       " '168.jpg': 'bridge',\n",
       " '169.jpg': 'bridge',\n",
       " '170.jpg': 'runway',\n",
       " '171.jpg': 'oil_well',\n",
       " '172.jpg': 'overpass',\n",
       " '173.jpg': 'swimming_pool',\n",
       " '174.jpg': 'overpass',\n",
       " '175.jpg': 'basketball_court',\n",
       " '176.jpg': 'crosswalk',\n",
       " '177.jpg': 'oil_well',\n",
       " '178.jpg': 'basketball_court',\n",
       " '179.jpg': 'crosswalk',\n",
       " '180.jpg': 'overpass',\n",
       " '181.jpg': 'overpass',\n",
       " '182.jpg': 'bridge',\n",
       " '183.jpg': 'overpass',\n",
       " '184.jpg': 'runway',\n",
       " '185.jpg': 'overpass',\n",
       " '186.jpg': 'railway',\n",
       " '187.jpg': 'basketball_court',\n",
       " '188.jpg': 'railway',\n",
       " '189.jpg': 'runway',\n",
       " '190.jpg': 'crosswalk',\n",
       " '191.jpg': 'overpass',\n",
       " '192.jpg': 'bridge',\n",
       " '193.jpg': 'overpass',\n",
       " '194.jpg': 'railway',\n",
       " '195.jpg': 'overpass',\n",
       " '196.jpg': 'basketball_court',\n",
       " '197.jpg': 'overpass',\n",
       " '198.jpg': 'golf_course',\n",
       " '199.jpg': 'runway',\n",
       " '200.jpg': 'runway'}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label_dict = {\r\n",
    "    \"basketball_court\": 1, \r\n",
    "    \"bridge\":2, \r\n",
    "    \"crosswalk\":3, \r\n",
    "    \"golf_course\":4, \r\n",
    "    \"oil_well\":5, \r\n",
    "    \"overpass\":6, \r\n",
    "    \"railway\":7, \r\n",
    "    \"runway\":8, \r\n",
    "    \"swimming_pool\":9, \r\n",
    "    \"tennis_court\":10\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Convert the predictions to labels from label_dict\r\n",
    "predictions_labels={}\r\n",
    "for key,value in predictions.items():\r\n",
    "    predictions_labels[key]=label_dict[value]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions_labels"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'101.jpg': 5,\n",
       " '102.jpg': 7,\n",
       " '103.jpg': 3,\n",
       " '104.jpg': 5,\n",
       " '105.jpg': 6,\n",
       " '106.jpg': 2,\n",
       " '107.jpg': 8,\n",
       " '108.jpg': 5,\n",
       " '109.jpg': 6,\n",
       " '110.jpg': 6,\n",
       " '111.jpg': 2,\n",
       " '112.jpg': 10,\n",
       " '113.jpg': 5,\n",
       " '114.jpg': 6,\n",
       " '115.jpg': 8,\n",
       " '116.jpg': 9,\n",
       " '117.jpg': 6,\n",
       " '118.jpg': 8,\n",
       " '119.jpg': 8,\n",
       " '120.jpg': 7,\n",
       " '121.jpg': 5,\n",
       " '122.jpg': 6,\n",
       " '123.jpg': 6,\n",
       " '124.jpg': 2,\n",
       " '125.jpg': 3,\n",
       " '126.jpg': 2,\n",
       " '127.jpg': 2,\n",
       " '128.jpg': 1,\n",
       " '129.jpg': 5,\n",
       " '130.jpg': 8,\n",
       " '131.jpg': 2,\n",
       " '132.jpg': 1,\n",
       " '133.jpg': 3,\n",
       " '134.jpg': 3,\n",
       " '135.jpg': 6,\n",
       " '136.jpg': 2,\n",
       " '137.jpg': 6,\n",
       " '138.jpg': 6,\n",
       " '139.jpg': 7,\n",
       " '140.jpg': 2,\n",
       " '141.jpg': 7,\n",
       " '142.jpg': 7,\n",
       " '143.jpg': 2,\n",
       " '144.jpg': 2,\n",
       " '145.jpg': 6,\n",
       " '146.jpg': 2,\n",
       " '147.jpg': 10,\n",
       " '148.jpg': 5,\n",
       " '149.jpg': 1,\n",
       " '150.jpg': 2,\n",
       " '151.jpg': 8,\n",
       " '152.jpg': 6,\n",
       " '153.jpg': 1,\n",
       " '154.jpg': 6,\n",
       " '155.jpg': 1,\n",
       " '156.jpg': 2,\n",
       " '157.jpg': 8,\n",
       " '158.jpg': 3,\n",
       " '159.jpg': 6,\n",
       " '160.jpg': 6,\n",
       " '161.jpg': 1,\n",
       " '162.jpg': 6,\n",
       " '163.jpg': 2,\n",
       " '164.jpg': 6,\n",
       " '165.jpg': 2,\n",
       " '166.jpg': 2,\n",
       " '167.jpg': 6,\n",
       " '168.jpg': 2,\n",
       " '169.jpg': 2,\n",
       " '170.jpg': 8,\n",
       " '171.jpg': 5,\n",
       " '172.jpg': 6,\n",
       " '173.jpg': 9,\n",
       " '174.jpg': 6,\n",
       " '175.jpg': 1,\n",
       " '176.jpg': 3,\n",
       " '177.jpg': 5,\n",
       " '178.jpg': 1,\n",
       " '179.jpg': 3,\n",
       " '180.jpg': 6,\n",
       " '181.jpg': 6,\n",
       " '182.jpg': 2,\n",
       " '183.jpg': 6,\n",
       " '184.jpg': 8,\n",
       " '185.jpg': 6,\n",
       " '186.jpg': 7,\n",
       " '187.jpg': 1,\n",
       " '188.jpg': 7,\n",
       " '189.jpg': 8,\n",
       " '190.jpg': 3,\n",
       " '191.jpg': 6,\n",
       " '192.jpg': 2,\n",
       " '193.jpg': 6,\n",
       " '194.jpg': 7,\n",
       " '195.jpg': 6,\n",
       " '196.jpg': 1,\n",
       " '197.jpg': 6,\n",
       " '198.jpg': 4,\n",
       " '199.jpg': 8,\n",
       " '200.jpg': 8}"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting the predictions to CSV format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert predictions to dataframe\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Make a dataframe with the predictions with column names as ImageID and LabelID\r\n",
    "predictions_df = pd.DataFrame.from_dict(predictions_labels, orient='index')\r\n",
    "\r\n",
    "# Removing the .jpg from the file names\r\n",
    "predictions_df.index=predictions_df.index.str.replace('.jpg','')\r\n",
    "\r\n",
    "# Naming the columns as ImageID and Label\r\n",
    "predictions_df.reset_index(level=0, inplace=True)\r\n",
    "predictions_df.columns = ['ImageID', 'LabelID']\r\n",
    "\r\n",
    "# Removing the index from the dataframe\r\n",
    "predictions_df.reset_index(drop= True, inplace=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Vinit\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>LabelID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>197</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>198</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>199</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageID  LabelID\n",
       "0      101        5\n",
       "1      102        7\n",
       "2      103        3\n",
       "3      104        5\n",
       "4      105        6\n",
       "..     ...      ...\n",
       "95     196        1\n",
       "96     197        6\n",
       "97     198        4\n",
       "98     199        8\n",
       "99     200        8\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Saving the predictions to csv\r\n",
    "predictions_df.to_csv('18D070067.csv',index=False)   #18D070067.csv is the name of the csv file and the index have been dropped"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '18D070067.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18676/1290782912.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Saving the predictions to csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'18D070067.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#18D070067.csv is the name of the csv file and the index have been dropped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3477\u001b[0m             \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m             \u001b[0mescapechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3479\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3480\u001b[0m         )\n\u001b[0;32m   3481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m         ) as handles:\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu-pytorch\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '18D070067.csv'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hence, the implementation of the model is complete and the predictions are converted to CSV format."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('gpu-pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "3d1484f6a5966aacf1d005944f88794f1e4469136ed9878e517c1d6594061a94"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}