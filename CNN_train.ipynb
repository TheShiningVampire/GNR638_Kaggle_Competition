{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<center><b><H1> KAGGLE COMPETITION GNR 638 </H1></b></center>\r\n",
    "<center><b><H2> VINIT AWALE </H2></b></center>\r\n",
    "<center><b><H2> 18D070067 </H2></b></center>\r\n",
    "<center><b><H2> ELECTRICAL DUAL DEGREE </H2></b></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the training folder to train and validation folders\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#import splitfolders\r\n",
    "\r\n",
    "#splitfolders.ratio('Dataset\\\\gnr6382021\\\\train', output='Dataset\\\\gnr6382021\\\\train', seed=1337, ratio=(.8, .2))  # 80% training, 20% validation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After this the folder structure is adjusted to have the following structure:\r\n",
    "\r\n",
    "\r\n",
    "```\r\n",
    "Dataset\r\n",
    "├── train\r\n",
    "│   ├──basketball_court\r\n",
    "│   ├──bridge\r\n",
    "│   ├──crosswalk   \r\n",
    "│   ├──golf_course\r\n",
    "│   ├──oil_well\r\n",
    "│   ├──overpass\r\n",
    "│   ├──railway\r\n",
    "|   ├──runway\r\n",
    "|   ├──swimming_pool\r\n",
    "|   ├──tennis_court\r\n",
    "|── val\r\n",
    "│   ├──basketball_court\r\n",
    "│   ├──bridge\r\n",
    "│   ├──crosswalk\r\n",
    "│   ├──golf_course\r\n",
    "│   ├──oil_well\r\n",
    "│   ├──overpass\r\n",
    "│   ├──railway\r\n",
    "|   ├──runway\r\n",
    "|   ├──swimming_pool\r\n",
    "|   ├──tennis_court\r\n",
    "└── test\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import glob\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "import torchvision\r\n",
    "from torchvision import transforms, utils\r\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\r\n",
    "import pathlib\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting the device"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transforms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transformer = transforms.Compose([\r\n",
    "    transforms.Resize((224,224)),\r\n",
    "    transforms.RandomHorizontalFlip(),    \r\n",
    "    transforms.ToTensor(),                  # make it into torch.Tensor and normalized into range [0, 1]\r\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # transform it into range [-1, 1]\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_path = \"Dataset\\\\train\"\r\n",
    "val_path = \"Dataset\\\\val\"\r\n",
    "\r\n",
    "\r\n",
    "train_loader = DataLoader(torchvision.datasets.ImageFolder(train_path, transform=transformer), batch_size=2, shuffle=True)\r\n",
    "val_loader = DataLoader(torchvision.datasets.ImageFolder(val_path, transform=transformer), batch_size=2, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Categories in the dataset\r\n",
    "root = pathlib.Path(train_path)\r\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the CNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "VGG_types = {\r\n",
    "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\r\n",
    "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\r\n",
    "    \"VGG16\": [64,64,\"M\",128,128,\"M\",256,256,256,\"M\",512,512,512,\"M\",512,512,512,\"M\"],\r\n",
    "    \"VGG19\": [64,64,\"M\",128,128,\"M\",256,256,256,256,\"M\",512,512,512,512,\"M\",512,512,512,512,\"M\"],\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "class VGG_net(nn.Module):\r\n",
    "    def __init__(self, in_channels, num_classes, type=\"VGG16\"):\r\n",
    "        super(VGG_net, self).__init__()\r\n",
    "        self.in_channels = in_channels\r\n",
    "        self.conv_layers = self.create_conv_layers(VGG_types[type])\r\n",
    "\r\n",
    "        self.fcs = nn.Sequential(\r\n",
    "            nn.Linear(512 * 7 * 7, 4096),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.5),\r\n",
    "            nn.Linear(4096, 4096),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.5),\r\n",
    "            nn.Linear(4096, num_classes),\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv_layers(x)\r\n",
    "        x = x.reshape(x.shape[0], -1)\r\n",
    "        x = self.fcs(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    def create_conv_layers(self, architecture):\r\n",
    "        layers = []\r\n",
    "        in_channels = self.in_channels\r\n",
    "\r\n",
    "        for x in architecture:\r\n",
    "            if type(x) == int:\r\n",
    "                out_channels = x\r\n",
    "\r\n",
    "                layers += [\r\n",
    "                    nn.Conv2d(\r\n",
    "                        in_channels=in_channels,\r\n",
    "                        out_channels=out_channels,\r\n",
    "                        kernel_size=(3, 3),\r\n",
    "                        stride=(1, 1),\r\n",
    "                        padding=(1, 1),\r\n",
    "                    ),\r\n",
    "                    nn.BatchNorm2d(x),\r\n",
    "                    nn.ReLU(),\r\n",
    "                ]\r\n",
    "                in_channels = x\r\n",
    "            elif x == \"M\":\r\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\r\n",
    "\r\n",
    "        return nn.Sequential(*layers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_class = len(classes) # Number of classes in the dataset\r\n",
    "\r\n",
    "model = VGG_net(3,num_class,\"VGG16\").to(device)\r\n",
    "print(model)\r\n",
    "## N = 3 (Mini batch size)\r\n",
    "# x = torch.randn(3, 3, 224, 224).to(device)\r\n",
    "# print(model(x).shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizer and Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\r\n",
    "loss_func = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Calculate number of training and validation images\r\n",
    "num_train = len(glob.glob(train_path + \"/**/*.jpg\"))\r\n",
    "num_val = len(glob.glob(val_path + \"/**/*.jpg\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Number of training images: {}\".format(num_train))\r\n",
    "print(\"Number of validation images: {}\".format(num_val))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training and saving the best model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_acc = 0.0\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    # Training and Evaluation on training dataset\r\n",
    "    model.train()\r\n",
    "    train_loss = 0.0\r\n",
    "    train_acc = 0.0\r\n",
    "    for i, (images, labels) in enumerate(train_loader):\r\n",
    "        images = images.to(device)\r\n",
    "        labels = labels.to(device)\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "\r\n",
    "        # Forward pass\r\n",
    "        outputs = model(images)\r\n",
    "        loss = loss_func(outputs, labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        train_loss += loss.cpu().data*images.size(0)\r\n",
    "        _, predicted = torch.max(outputs.data, 1)\r\n",
    "        train_acc += int(torch.sum(predicted == labels.data))\r\n",
    "\r\n",
    "    train_loss = train_loss / num_train\r\n",
    "    train_acc = train_acc / num_train\r\n",
    "\r\n",
    "    # Evaluation on validation dataset\r\n",
    "    model.eval() \r\n",
    "    val_loss = 0.0\r\n",
    "    val_acc = 0.0\r\n",
    "    for i, (images, labels) in enumerate(val_loader):\r\n",
    "        images = images.to(device)\r\n",
    "        labels = labels.to(device)\r\n",
    "\r\n",
    "        # Forward pass\r\n",
    "        outputs = model(images)\r\n",
    "        _, predicted = torch.max(outputs.data, 1)\r\n",
    "        val_acc += int(torch.sum(predicted == labels.data))\r\n",
    "    \r\n",
    "    val_acc = val_acc / num_val\r\n",
    "\r\n",
    "    print(\"Epoch: \" + str(epoch + 1) + \", Training Loss: \" + str(train_loss) + \", Training Accuracy: \" + str(train_acc) + \", Validation Accuracy: \" + str(val_acc))\r\n",
    "\r\n",
    "    if val_acc > best_acc:\r\n",
    "        best_acc = val_acc                                     # Save the best accuracy   \r\n",
    "        torch.save(model.state_dict(), \"VGG16_best_model.pt\")  # Save the model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using transfer learning of VGG16 from models on our dataset\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('gpu-pytorch': conda)"
  },
  "interpreter": {
   "hash": "3d1484f6a5966aacf1d005944f88794f1e4469136ed9878e517c1d6594061a94"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}